n <- 30 # n observations
B <- 1000 # B simulations
set.seed(1234)
# sample generations
samples <- array(0, dim = c(n, 3, B))
for(i in 1:B){
samples[, 1, i] <-rnorm(n, 0, 1)
samples[, 2, i] <-rt(n, df = 3)
samples[, 3, i] <-runif(n, 0, 1)
}
#Sample statistics, collect values for sample mean
# and sample variance for each distribution
samples_stat <- array(0, dim = c(2, 3, B))
for(j in 1:3){
#sample mean
samples_stat[1, j, ] <- apply(samples[,j,], 2, mean) # 2 in apply means the function as to be applied to columns
#sample variance
samples_stat[2, j, ] <- apply(samples[,j,], 2, var)
}
##########
# N(0,1) #
##########
par(mfrow = c(1, 3))
hist.scott(samples_stat[1,1,], prob =TRUE, xlab = "x", main = "N(0,1)")
curve(dnorm(x, 0, 1/sqrt(n)), col = "red", lwd = 2, add = TRUE)
# the distribution of the sample mean is a normal distribution
# as well, with variance 1/sqrt(n)
########
# t(3) #
########
hist.scott(samples_stat[1,2,], prob =TRUE,
xlab = "x", main = "t3")
curve(dnorm(x, 0, sqrt(3/n)), col = "red",
lwd = 2, add = TRUE)
##########
# U(0,1) #
##########
hist.scott(samples_stat[1,3,], prob =TRUE,
xlab = "x", main = "U(0,1)")
curve(dnorm(x, 1/2, 1/sqrt(12*n)), col = "red",
lwd = 2, add = TRUE)
#######################################
# Distribution of the sample variance #
#######################################
sigma2_n <- 1
sigma2_t <- 3 #3/(3-2)
sigma2_u <- 1/12
par(mfrow=c(1,3))
##########
# N(0,1) #
##########
hist.scott(samples_stat[2,1, ], prob=TRUE, xlab = expression(s^2), main = "N(0,1)")
curve((n - 1)/sigma2_n * dchisq(x * (n-1)/sigma2_n, df = n - 1), add=TRUE, col="red", lwd=2)
########
# t(3) #
########
hist.scott(samples_stat[2,2, ], prob=TRUE,
xlab = expression(s^2), main = "t(3)",
xlim = c(0,25))
curve((n - 1)/sigma2_t * dchisq(x * (n - 1)/sigma2_t,
df = n - 1),
add=TRUE, col="red", lwd=2)
var_sample_var <- var(samples_stat[2,2, ])
curve(2 * sigma2_t /var_sample_var * dchisq(x * 2 * sigma2_t/var_sample_var ,
df = (2*sigma2_t^2/var_sample_var)),
add=TRUE, col="green", lwd=2)
##########
# U(0,1) #
##########
hist.scott(samples_stat[2,3, ],  prob=TRUE, xlab = expression(s^2), main = "U(0,1)")
curve((n-1)/sigma2_u *dchisq(x * (n-1)/sigma2_u, df = n - 1), add=TRUE, col="red", lwd=2)
var_sample_var <- var(samples_stat[2,3, ])
curve(2*sigma2_u/var_sample_var * dchisq(x * 2 * sigma2_u/var_sample_var,
df = (2 * sigma2_u^2/var_sample_var)),
add=TRUE, col="green", lwd=2)
set.seed(1234)
B <- 1000
n <- 10
mu <- 5
sigma <- 2
s2 <- rep(0, B)
s2_b <- rep(0, B)
set.seed(1234)
B <- 1000
n <- 10
mu <- 5
sigma <- 2
s2 <- rep(0, B)
s2_b <- rep(0, B)
for(i in 1:B) {
x <- rnorm(n,5) # normally distributed values
s2[,i] <- (1/(n-1))*sum((x - mean(x))^2)
s2_b[,i] <- (1/n)*sum((x - mean(x))^2)
}
for(i in 1:B) {
x <- rnorm(n,5) # normally distributed values
s2[i] <- (1/(n-1))*sum((x - mean(x))^2)
s2_b[i] <- (1/n)*sum((x - mean(x))^2)
}
var(x)
mean(s2)
mean(s2_b)
for(i in 1:B) {
x <- rnorm(n,5) # normally distributed values
#s2[i] <- (1/(n-1))*sum((x - mean(x))^2)
#s2_b[i] <- (1/n)*sum((x - mean(x))^2)
# or
s2[i] <- var(y)
s2_b[i] <-  s2[i] *(n - 1)/n
}
mean(s2)
mean(s2_b)
n <- 100
mu <- 5
sigma <- 2
s2 <- rep(0, B)
s2_b <- rep(0, B)
for(i in 1:B) {
x <- rnorm(n,5) # normally distributed values
s2[i] <- (1/(n-1))*sum((x - mean(x))^2)
s2_b[i] <- (1/n)*sum((x - mean(x))^2)
# or
#s2[i] <- var(y)
#s2_b[i] <-  s2[i] *(n - 1)/n
}
mean(s2)
mean(s2_b)
n <- 1000
for(i in 1:B) {
x <- rnorm(n,5) # normally distributed values
s2[i] <- (1/(n-1))*sum((x - mean(x))^2)
s2_b[i] <- (1/n)*sum((x - mean(x))^2)
# or
#s2[i] <- var(y)
#s2_b[i] <-  s2[i] *(n - 1)/n
}
mean(s2)
par(mfrow=c(1,2))
s2_m <- mean(s2)
s2_b_m <- mean(s2_b)
par(mfrow = c(1,2))
hist.scott(s2, xlab = expression(s^2), main = expression(s^2), prob = TRUE)
abline(v = sigma^2, col = "red")
sigm^2
sigma^2
set.seed(1234)
B <- 1000
n <- 1000
mu <- 5
sigma <- 2
s2 <- rep(0, B)
s2_b <- rep(0, B)
for(i in 1:B) {
x <- rnorm(n,mu,sigma) # normally distributed values
s2[i] <- (1/(n-1))*sum((x - mean(x))^2)
s2_b[i] <- (1/n)*sum((x - mean(x))^2)
# or
#s2[i] <- var(y)
#s2_b[i] <-  s2[i] *(n - 1)/n
}
par(mfrow=c(1,2))
s2_m <- mean(s2)
s2_b_m <- mean(s2_b)
var(x)
par(mfrow = c(1,2))
hist.scott(s2, xlab = expression(s^2), main = expression(s^2), prob = TRUE)
par(mfrow = c(1,2))
hist.scott(s2, xlab = expression(s^2), main = expression(s^2), prob = TRUE)
abline(v = sigma^2, col = "red", lwd=2)
abline(v = s2_m, col = "blue", lty=2)
hist.scott(s2_b, xlab = expression(s[b]^2),
main = expression(s[b]^2), prob = TRUE)
abline(v = sigma^2, col = "red")
abline(v = s2_b_m, col = "blue", lty=2)
### test statistic
# given n iid random and normally distributed variables,
# provide a test statistic for the available sample
set.seed(1234)
B <- 1000
n <- 100
mu <- 3
sigma <- 1
test_statistic <- rep(0, B)
for(i in 1:B) {
x <- rnorm(n,mu,sigma) # normally distributed values
test_statistic <- sqrt(n)*mean(x)
}
for(i in 1:B) {
x <- rnorm(n,mu,sigma) # normally distributed values
test_statistic[i] <- sqrt(n)*mean(x)
}
hist(test_statistic)
tt <- seq(min(test_statistic), max(test_statistic), l = 1000)
tt <- seq(min(test_statistic), max(test_statistic), l = 1000)
tt <- seq(min(test_statistic), max(test_statistic), l = 1000)
lines(tt, dnorm(tt,mu), type='l',add=T)
hist.scott(test_statistic)
tt <- seq(min(test_statistic), max(test_statistic), l = 1000)
lines(tt, dnorm(tt,mu), type='l')
tt <- seq(min(test_statistic), max(test_statistic), l = 1000)
tt <- seq(min(test_statistic), max(test_statistic), l = 1000)
lines(tt, dnorm(tt,mu), type='l')
lines(tt, dnorm(tt,mu,sigma), type='l')
lines(tt, dnorm(tt,mu,sigma), type='l', col=2, lwd=2)
lines(tt, dnorm(tt,sqrt(n)*mean(x),sigma), type='l', col=2, lwd=2)
set.seed(13);
n <- 10;
y_obs <- rnorm(n)
set.seed(13);
n <- 10;
y_obs <- rnorm(n)
z_obs <- mean(y_obs) * sqrt(n) print(z_obs)
z_obs <- mean(y_obs) * sqrt(n)
print(z_obs)
n <- 100;
y_obs <- rnorm(n)
z_obs <- mean(y_obs) * sqrt(n)
print(z_obs)
n <- 1000;
y_obs <- rnorm(n)
z_obs <- mean(y_obs) * sqrt(n)
print(z_obs)
n <- 10000;
y_obs <- rnorm(n)
z_obs <- mean(y_obs) * sqrt(n)
print(z_obs)
n <- 10;
y_obs <- rnorm(n)
z_obs <- mean(y_obs) * sqrt(n)
print(z_obs)
M <- 100000;
z_sim <- numeric(M)
for(i in 1:M) {
y <- rnorm(n)
z_sim[i] <- mean(y) * sqrt(n)
}
c(mean(z_sim >= z_obs), 1 - pnorm(z_obs))
z_sim >= z_obs
sum(z_sim >= z_obs)
sum(z_sim >= z_obs)/length(z_sim)
length(z_sim)
(mean(z_sim >= z_obs)
)
n <- 10;
y_obs <- rnorm(n)
z_obs <- mean(y_obs) * sqrt(n)
print(z_obs)
M <- 100000;
z_sim <- numeric(M)
for(i in 1:M) {
y <- rnorm(n)
z_sim[i] <- mean(y) * sqrt(n)
}
# compute p values
# per il primo prendiamo
c(mean(z_sim >= z_obs), 1 - pnorm(z_obs))
set.seed(13);
n <- 10;
y_obs <- rnorm(n)
z_obs <- mean(y_obs) * sqrt(n)
print(z_obs)
M <- 100000;
z_sim <- numeric(M)
for(i in 1:M) {
y <- rnorm(n)
z_sim[i] <- mean(y) * sqrt(n)
}
c(mean(z_sim >= z_obs), 1 - pnorm(z_obs))
mean(y_obs)
#[1] 0.02877000 0.02887856
#null hypothesis seems to be acceptable
layout(1)
hist.scott(y)
for(i in 1:M) {
y <- rnorm(n)
z_sim[i] <- mean(y) * sqrt(n)
}
hist.scott(y, main="")
length(y)
y
mean(y)
hist.scott(z_sim, main="")
yy <- seq(min(zsim),max(zsim), l=1000)
yy <- seq(min(z_sim),max(z_sim), l=1000)
lines(yy, dnorm(yy,0), add=T)
lines(yy, dnorm(yy,0), add=T, col=2)
# zobs
abline(v=z_obs, lwd=2, col=3)
1 - pnorm(1)
1 - pnorm(1)
require(MESS)
install.packages("MESS")
require(MESS)
auc(z_obs:4,dnorm(z_obs:4,0), type = 'spline')
interval <- seq(z_obs,4, l=1000)
auc(interval,dnorm(interval,0), type = 'spline')
M <- 10000;
n <- 1000;
mat.ci <- matrix(NA, nrow = M, ncol = 2)
for(i in 1:M) {
# simulate sample
y <- rnorm(n, 5)
# building interval for the given sample
se_t <- sqrt(var(y)/n) * qt(0.975, n-1) # alpha 0.5, alpha/2 = 0.025
mat.ci[i,] <- mean(y) + se_t * c(-1, 1)
}
# compute the mean of the times in which the true mean (5)
# is in the computed confidence interval [a,b]
a <- mat.ci[,1]
b <- mat.ci[,2]
mean(a < 5 & 5 < b)
layout(1)
hist.scott(y)
library(MASS)
hist.scott(y)
xx <- seq(min(y), max(y), l = 1000)
lines(xx,dnorm(xx,5),type='l', col=2,lwd = 2)
lines(density(y), type='l', col=3, lwd = 2)
#confidence interval
abline(v = mean(mat.ci[,1]), col = 4, lwd=2)
abline(v = mean(mat.ci[,2]), col = 4, lwd=2)
#plot the mean for each interval
plot(rep(5, 30), 1:30, pch = 16, ylab="Sample", xlab=expression(mu), xlim= c(4.8,5.2))
color = 1
for(i in 1:30) {
if(mat.ci[i,1] > 5 | mat.ci[i,2] < 5) {
color = 3
}
segments(mat.ci[i,1],i,mat.ci[i,2],i, col = color, lwd=2)
color = 1
}
### approximate intervals computed for samples of different
### sizes n = 10, n = 100
M <- 100000;
n <- 10;
mat.ci <- matrix(NA, nrow = M, ncol = 2)
for(i in 1:M) {
y <- rnorm(n, 5)
se_z <- sqrt(var(y) / n) * qnorm(0.975)
mat.ci[i,] <- mean(y) + se_z * c(-1, 1)
}
mean(mat.ci[,1] < 5 & mat.ci[,2] > 5)
# shape = 0.5; scale = 1
curve(dweibull(x, shape=0.5, scale = 1), from = 0, to = 3,
ylab=expression("f(y;" ~ gamma ~"," ~ beta ~")"), xlab="y")
# shape = 1; scale = 1
curve(dweibull(x, shape=1, scale = 1), add=TRUE, col = "red")
# shape = 1; scale = 2
curve(dweibull(x, shape=1, scale = 2), add=TRUE, col = "blue")
# shape = 1.5; scale = 1
curve(dweibull(x, shape=1.5, scale = 1), add=TRUE, col = "green")
# shape = 5; scale = 1
curve(dweibull(x, shape=5, scale = 1), add=TRUE, col = "purple")
# add a legend to plot
legend("topright", legend=c(expression(gamma == 0.5 * ";" ~ beta == 1),
expression(gamma == 1 * ";" ~ beta == 1),
expression(gamma == 1 * ";" ~ beta == 2),
expression(gamma == 1.5 * ";" ~ beta == 1),
expression(gamma == 5 * ";" ~ beta == 1)),
col = c("black", "red", "blue", "green", "purple"), lty = rep(1,5),
cex = 0.8, box.lty = 0)
# by using the dweibull() function
# -sum of log of distribution function
n_logLik_Weib <- function(param, data){
-sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
# by hand (the last term in the summation, sum(log(y)), is the additive constant which could be removed without affecting inferential results)
n_logLik_Weib2 <- function(param, data){
n <- length(data)
res <- n*log(param[1]) - n*param[1]*log(param[2]) +
(param[1]-1)*sum(log(data)) - sum((data/param[2])^param[1])
return(-res)
}
theta <- c(2,2)
set.seed(1)
y <- rweibull(15, shape = 7, scale = 155)
n_logLik_Weib(theta, data = y)
n_logLik_Weib2(theta, data = y)
param <- c(gammahat, betahat)
# MLE of gamma: solve the lok-likelihood equation based on the previous score by using the uniroot function
# The function uniroot searches the interval from lower to upper for a root (i.e., zero) of
# the function f with respect to its first argument.
gammahat <- uniroot(logLik_score_g, c(1e-5, 15), data = y)$root
gammahat
# Score function (derivatives of l(theta) w.r.t. to gamma evaluated in betahat)
# substitute beta in the first equation of log likelihood derivative in order to obtain
# an equation which depends just on gamma
logLik_score_g <- function(gamms, data){
n <- length(data)
res <- n/gamma + sum(log(data)) - n*(sum(data^gamma*log(data))/(sum(data^gamma)))
return(res)
}
# MLE of gamma: solve the lok-likelihood equation based on the previous score by using the uniroot function
# The function uniroot searches the interval from lower to upper for a root (i.e., zero) of
# the function f with respect to its first argument.
gammahat <- uniroot(logLik_score_g, c(1e-5, 15), data = y)$root
gammahat
# Score function (derivatives of l(theta) w.r.t. to gamma evaluated in betahat)
# substitute beta in the first equation of log likelihood derivative in order to obtain
# an equation which depends just on gamma
logLik_score_g <- function(gamma, data){
n <- length(data)
res <- n/gamma + sum(log(data)) - n*(sum(data^gamma*log(data))/(sum(data^gamma)))
return(res)
}
# MLE of gamma: solve the lok-likelihood equation based on the previous score by using the uniroot function
# The function uniroot searches the interval from lower to upper for a root (i.e., zero) of
# the function f with respect to its first argument.
gammahat <- uniroot(logLik_score_g, c(1e-5, 15), data = y)$root
gammahat
# MLE of beta, for a fixed value of gamma (the MLE of gamma)
betahat <- mean(y^gammahat)^(1/gammahat)
betahat
param <- c(gammahat, betahat)
n_logLik_Weib2(param, y)
# By using nlm function
# Staring values near to MLE estimate
weib.nlm_start1 <- nlm(f = n_logLik_Weib, p = c(5, 160), data = y, hessian = TRUE)
weib.nlm_start1
?nlm
# By using nlm function
# Staring values near to MLE estimate
weib.nlm_start1 <- nlm(f = n_logLik_Weib, p = c(6, 157), data = y, hessian = TRUE)
# By using nlm function
# Staring values near to MLE estimate
weib.nlm_start1 <- nlm(f = n_logLik_Weib, p = c(6, 157), data = y, hessian = TRUE)
weib.nlm_start1
# By using nlm function
# Staring values near to MLE estimate
weib.nlm_start1 <- nlm(f = n_logLik_Weib, p = c(6, 156), data = y, hessian = TRUE)
#Staring values far to MLE estimate
weib.nlm_start2 <- nlm(f = n_logLik_Weib, p = c(0.1, 0.1), data = y, hessian = TRUE)
weib.nlm_start2
# By using optim function
#Staring values near to MLE estimate
weib.optim_start1 <- optim(par = c(5, 160), fn= n_logLik_Weib, data = y, hessian = TRUE,
method = "L-BFGS-B", lower = rep(1e-7, 2), upper = rep(Inf,2))
weib.optim_start1
# What happens if we using c(0,0) as guesstimate
weib.nlm_start3 <- nlm(f = n_logLik_Weib, p = c(0, 0), data = y, hessian = TRUE)
weib.nlm_start3
# Negative log-likelihood
n_logLik_Weib_rep <- function(param, data) n_logLik_Weib(theta(param), data)
# Optimize the log-likelihood function by using nlm (also here the are some warnings but the algorithm works)
weib_nlm_start3_rep <- nlm(f = n_logLik_Weib_rep, p = c(0, 0), data = y)
weib_nlm_start3_rep
# Reparameterization
theta <- function(omega) exp(omega)
# Negative log-likelihood
n_logLik_Weib_rep <- function(param, data) n_logLik_Weib(theta(param), data)
# Optimize the log-likelihood function by using nlm (also here the are some warnings but the algorithm works)
weib_nlm_start3_rep <- nlm(f = n_logLik_Weib_rep, p = c(0, 0), data = y)
weib_nlm_start3_rep
exp
euler
2.71^5.1
2.71^2.0432
2.71^5.0313
# Check
theta(weib_nlm_start3_rep$estimate)
weib_nlm_start3_rep
View(weib.nlm_start3)
weib.nlm_start3[["estimate"]]
weib.nlm_start3[["estimate"]]
# Check
theta(weib_nlm_start3_rep$estimate)
weib_nlm_start3_rep
View(weib_nlm_start3_rep)
# which is the most likely p to be associated with the full box?
# we can write the likelihood function (prob. of observing x high quality seeds)
# as follows: (for each p)
L <- choose(30,x)*p^x*(1-p)^{30-x}
L
p <- c(.5, .7, .8, .9); #percentage of high quality seeds in one box
n <- 30 # size of considered sample
x=23 # assume we have 23 high quality seeds upon the 30 considered
# which is the most likely p to be associated with the full box?
# we can write the likelihood function (prob. of observing x high quality seeds)
# as follows: (for each p)
L <- choose(30,x)*p^x*(1-p)^{30-x}
L
## [1] 0.001895986 0.121853726 0.153820699 0.018043169
plot(p,L,type="h",main="likelihood function", cex.lab=0.7, cex.axis=0.5, ylab="likelihood", ylim=c(0,0.2), col=2)
prior <- c(.4, .3, .2, .1)
posterior <- prior*L/sum(prior*L)
posterior
## [1] 0.01085235 0.52310482 0.44022371 0.02581912 plot(p,posterior,type="h", main="posterior", cex.lab=0.7,
cex.axis=0.5, ylab="posterior prob ", ylim=c(0,0.6), col=2)
plot(p,posterior,type="h", main="posterior", cex.lab=0.7,
cex.axis=0.5, ylab="posterior prob ", ylim=c(0,0.6), col=2)
posterior <- prior*px/sum(prior*px)
plot(p,posterior,type="h", main="posterior", cex.lab=0.7,
cex.axis=0.5, ylab="posterior prob ", ylim=c(0,0.6), col=2)
posterior <- prior*px/sum(prior*px)
# else
# P(x|pi) is:
px <- c(0,0,1/3,0)
posterior <- prior*px/sum(prior*px)
#plot likelihood function
n <- 30
z <- 23
curve(x^z*(1-x)^(30-z), xlim=c(0,1), xlab="p",ylab="L(p)")
x
rm x
rm(x)
curve(x^z*(1-x)^(30-z), xlim=c(0,1), xlab="p",ylab="L(p)")
x
setwd("~/Documents/hpc_assignment1/section2")
core_ucx <- data.frame(read.csv("core_ucx.csv"))
View(core_ucx)
attach(core_ucx)
plot(X.bytes, t.usec., type='l')
